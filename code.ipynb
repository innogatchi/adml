{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from glob import glob\n",
    "import pickle\n",
    "\n",
    "# Initialize SKIMAGE_AVAILABLE to False\n",
    "SKIMAGE_AVAILABLE = False\n",
    "\n",
    "# Try to import scikit-image\n",
    "try:\n",
    "    from skimage.feature import local_binary_pattern, hog\n",
    "    from skimage.color import rgb2gray\n",
    "    SKIMAGE_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"scikit-image is not installed. LBP and HOG features will be skipped.\")\n",
    "\n",
    "# Initialize TENSORFLOW_AVAILABLE to False\n",
    "TENSORFLOW_AVAILABLE = False\n",
    "\n",
    "# Try to import TensorFlow\n",
    "try:\n",
    "    from tensorflow.keras.applications import VGG16\n",
    "    from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "    from tensorflow.keras.models import Model\n",
    "    base_model = VGG16(weights='imagenet', include_top=False)\n",
    "    model = Model(inputs=base_model.input, outputs=base_model.get_layer('block5_pool').output)\n",
    "    TENSORFLOW_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"TensorFlow is not installed. Deep features will be skipped.\")\n",
    "\n",
    "IMAGE_FOLDER = \"E:/Coding/Advanced ML/train_data\"\n",
    "LABELS_FILE = \"E:/Coding/Advanced ML/train.csv\"\n",
    "IMG_SIZE = (64, 64)\n",
    "PCA_CACHE_FILE = \"pca_results_combined.pkl\"  # Updated cache file for PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Loading Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder, size=IMG_SIZE):\n",
    "    images = []\n",
    "    filenames = []\n",
    "    for filename in glob(os.path.join(folder, \"*.jpg\")):  # Or other extensions\n",
    "        img = cv2.imread(filename)\n",
    "        if img is not None:  # Check if image was read successfully\n",
    "            img = cv2.resize(img, size)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            images.append(img)\n",
    "            filenames.append(os.path.basename(filename))\n",
    "        else:\n",
    "            print(f\"Warning: Could not read image {filename}. Skipping.\")\n",
    "    return images, filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the RGBW, LBP, Edge Features, HOG, Color Histogram, Deep Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rgbw_features(image):\n",
    "    r, g, b = cv2.split(image)\n",
    "    w = (r + g + b) / 3  # White component as average of RGB\n",
    "    return np.array([r.mean(), g.mean(), b.mean(), w.mean()])\n",
    "\n",
    "def extract_lbp_features(image):\n",
    "    gray = rgb2gray(image)\n",
    "    lbp = local_binary_pattern(gray, P=8, R=1, method=\"uniform\")\n",
    "    hist, _ = np.histogram(lbp, bins=np.arange(0, 10), range=(0, 9))\n",
    "    hist = hist.astype(\"float\")\n",
    "    hist /= (hist.sum() + 1e-6)  # Normalize the histogram\n",
    "    return hist\n",
    "\n",
    "def extract_edge_features(image):\n",
    "    gray = rgb2gray(image)\n",
    "    edges = cv2.Canny((gray * 255).astype(np.uint8), 100, 200)\n",
    "    return edges.mean()\n",
    "\n",
    "def extract_hog_features(image):\n",
    "    if not SKIMAGE_AVAILABLE:\n",
    "        return np.array([])  # Return an empty array if scikit-image is not available\n",
    "    gray = rgb2gray(image)\n",
    "    fd, _ = hog(gray, orientations=8, pixels_per_cell=(16, 16),\n",
    "                cells_per_block=(1, 1), visualize=True, channel_axis=None)  # Use channel_axis instead of multichannel\n",
    "    return fd\n",
    "\n",
    "def extract_color_histogram(image, bins=(8, 8, 8)):\n",
    "    hist = cv2.calcHist([image], [0, 1, 2], None, bins, [0, 256, 0, 256, 0, 256])\n",
    "    cv2.normalize(hist, hist)\n",
    "    return hist.flatten()\n",
    "\n",
    "def extract_deep_features(image, model):\n",
    "    image = cv2.resize(image, (224, 224))  # Resize for VGG16\n",
    "    image = preprocess_input(image)\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    features = model.predict(image)\n",
    "    return features.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained VGG16 model + higher level layers\n",
    "base_model = VGG16(weights='imagenet', include_top=False)\n",
    "model = Model(inputs=base_model.input, outputs=base_model.get_layer('block5_pool').output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df = pd.read_csv(LABELS_FILE)\n",
    "labels_dict = dict(zip(labels_df['file_name'], labels_df['label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process Images and Perform PCA with Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import time\n",
    "\n",
    "def extract_features(img):\n",
    "    rgbw = extract_rgbw_features(img)\n",
    "    lbp = extract_lbp_features(img)\n",
    "    edge = extract_edge_features(img)\n",
    "    hog_feat = extract_hog_features(img)\n",
    "    color_hist = extract_color_histogram(img)\n",
    "    if TENSORFLOW_AVAILABLE:\n",
    "        deep_feat = extract_deep_features(img, model)\n",
    "    else:\n",
    "        deep_feat = np.array([])  # Skip deep features if TensorFlow is not available\n",
    "    return np.hstack([rgbw, lbp, edge, hog_feat, color_hist, deep_feat])\n",
    "\n",
    "def process_images_and_pca(image_folder, labels_dict, img_size=IMG_SIZE, cache_file=PCA_CACHE_FILE):\n",
    "    if os.path.exists(cache_file):\n",
    "        print(\"Loading PCA results from cache...\")\n",
    "        with open(cache_file, 'rb') as f:\n",
    "            df_pca, df_features, scaler, pca = pickle.load(f)\n",
    "            return df_pca, df_features, scaler, pca\n",
    "    else:\n",
    "        print(\"Processing images and performing PCA...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Load images\n",
    "        images, filenames = load_images_from_folder(image_folder, img_size)\n",
    "        print(f\"Loaded {len(images)} images in {time.time() - start_time:.2f} seconds.\")\n",
    "        \n",
    "        # Parallel feature extraction\n",
    "        feature_extraction_start = time.time()\n",
    "        features_list = Parallel(n_jobs=-1)(delayed(extract_features)(img) for img in images)\n",
    "        print(f\"Feature extraction completed in {time.time() - feature_extraction_start:.2f} seconds.\")\n",
    "        \n",
    "        # Create DataFrame\n",
    "        features = np.array(features_list)\n",
    "        df_features = pd.DataFrame(features)\n",
    "        df_features['Image'] = filenames\n",
    "        df_features['Label'] = df_features['Image'].map(lambda x: labels_dict.get(f\"train_data/{x}\", np.nan))\n",
    "        \n",
    "        # Standardize features\n",
    "        scaler = StandardScaler()\n",
    "        features_scaled = scaler.fit_transform(features)\n",
    "        print(f\"Feature scaling completed in {time.time() - start_time:.2f} seconds.\")\n",
    "        \n",
    "        # Perform PCA\n",
    "        pca = PCA(n_components=20)\n",
    "        pca_result = pca.fit_transform(features_scaled)\n",
    "        print(f\"PCA completed in {time.time() - start_time:.2f} seconds.\")\n",
    "        \n",
    "        # Create PCA DataFrame\n",
    "        df_pca = pd.DataFrame(pca_result, columns=['PC1', 'PC2'])\n",
    "        df_pca['Image'] = filenames\n",
    "        df_pca['Label'] = df_features['Label']\n",
    "        \n",
    "        # Cache results\n",
    "        with open(cache_file, 'wb') as f:\n",
    "            pickle.dump((df_pca, df_features, scaler, pca), f)\n",
    "        print(f\"Total time: {time.time() - start_time:.2f} seconds.\")\n",
    "        \n",
    "        return df_pca, df_features, scaler, pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing images and performing PCA...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_pca, df_features, scaler, pca \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_images_and_pca\u001b[49m\u001b[43m(\u001b[49m\u001b[43mIMAGE_FOLDER\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[22], line 27\u001b[0m, in \u001b[0;36mprocess_images_and_pca\u001b[1;34m(image_folder, labels_dict, img_size, cache_file)\u001b[0m\n\u001b[0;32m     24\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Load images\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m images, filenames \u001b[38;5;241m=\u001b[39m \u001b[43mload_images_from_folder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(images)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m images in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Parallel feature extraction\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[16], line 5\u001b[0m, in \u001b[0;36mload_images_from_folder\u001b[1;34m(folder, size)\u001b[0m\n\u001b[0;32m      3\u001b[0m filenames \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m glob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)):  \u001b[38;5;66;03m# Or other extensions\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m img \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# Check if image was read successfully\u001b[39;00m\n\u001b[0;32m      7\u001b[0m         img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(img, size)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_pca, df_features, scaler, pca = process_images_and_pca(IMAGE_FOLDER, labels_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elbow Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue with clustering and evaluation as before\n",
    "inertia = []\n",
    "cluster_range = range(1, 30)\n",
    "\n",
    "for k in cluster_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(df_pca[['PC1', 'PC2']])\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.plot(cluster_range, inertia, marker='o', linestyle='--')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Inertia (Sum of Squared Distances)')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Silhouette Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "sil_scores = []\n",
    "\n",
    "for k in range(2, 10):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(df_pca[['PC1', 'PC2']])\n",
    "    score = silhouette_score(df_pca[['PC1', 'PC2']], cluster_labels)\n",
    "    sil_scores.append(score)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(2, 10), sil_scores, marker='o', linestyle='--')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score for Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "df_pca['Cluster'] = kmeans.fit_predict(df_pca[['PC1', 'PC2']])\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x='PC1', y='PC2', hue=df_pca['Cluster'].astype(str), palette='coolwarm', data=df_pca)\n",
    "plt.title('K-Means Clustering on PCA of Combined Image Features')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
